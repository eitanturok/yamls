max_seq_len: 3000
seed: 1
precision: amp_bf16

models:
-
  model_name: mosaicml/mpt-7b-8k
  model:
    name: hf_causal_lm
    pretrained_model_name_or_path: mosaicml/mpt-7b-8k
    init_device: mixed
    pretrained: true
    # load_in_8bit: true
    config_overrides:
      max_seq_len: ${max_seq_len}
  tokenizer:
    name: mosaicml/mpt-7b
    kwargs:
      model_max_length: ${max_seq_len}


device_eval_batch_size: 1

# FSDP config for model sharding
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: FULL
  forward_prefetch: True
  limit_all_gathers: True

icl_tasks:
-
  label: ms-marco
  # dataset_uri: /mnt/workdisk/eitan/llm-foundry/scripts/eval/local_data/rag_generation/ms_marco_validation.jsonl
  dataset_uri: eitanturok/ms-marco
  hf_loading_vars:
    split: small
  num_fewshot: [0]
  icl_task_type: rag_generation
  prompt_string: "Please use the passages to answer the question:\n"
  continuation_delimiter: "\nAnswer: " # this separates questions from answers

eval_gauntlet:
  weighting: EQUAL
  subtract_random_baseline: true
  rescale_accuracy: true
  averages:
    core_average:
    - rag_generation
  categories:
  - name: rag_generation
    benchmarks:
    - name: ms-marco
      num_fewshot: 0
      random_baseline: 0