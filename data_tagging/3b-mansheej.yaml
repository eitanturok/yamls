command: |-
  export NCCL_COLLNET_ENABLE="0"
  export NCCL_IB_PCI_RELAXED_ORDERING="1"

  export AWS_PROFILE=data-force-one
  export MOSAICML_PLATFORM=FALSE
  export CUDA_MODULE_LOADING=LAZY

  cd llm-foundry/scripts
  composer eval/eval.py  /mnt/config/parameters.yaml || (echo "Command failed - killing python" && pkill python && exit 1)

image: mosaicml/llm-foundry:2.1.0_cu121-latest
integrations:
- integration_type: git_repo
  git_repo: mosaicml/llm-foundry
  git_branch: main
  pip_install: .[gpu,openai]
  ssh_clone: false

env_variables:
- key: AWS_PROFILE
  value: data-force-one

scheduling:
  priority: low
  preemptible: true

parameters:
  loggers:
    wandb: {}

  data_local: /root/my-copy
  tokenizer_name: tiktoken
  max_seq_len: 4096
  global_seed: 17

  # Run Name
  run_name: # If left blank, will be read from env var $COMPOSER_RUN_NAME
  # Model
  models:
  -
    model_name: 3b-eval
    # Tokenizer
    tokenizer:
      name: tiktoken
      kwargs:
        model_name: gpt-4

    # Model
    model:
      name: mpt_causal_lm
      init_device: meta
      tokenizer_name: ${tokenizer_name}
      max_seq_len: ${max_seq_len}
      vocab_size: 100352
      no_bias: True
      norm_type: low_precision_layernorm

      # added settings
      expansion_ratio: 4

      attn_config:
        attn_type: grouped_query_attention
        attn_impl: triton
        clip_qkv: 8
        alibi: True
        attn_uses_sequence_id: false


  device_eval_batch_size: 1
  precision: amp_bf16
  # FSDP
  fsdp_config:
    activation_checkpointing: false
    activation_checkpointing_reentrant: true
    activation_cpu_offload: false
    backward_prefetch: BACKWARD_POST
    forward_prefetch: true
    limit_all_gathers: true
    mixed_precision: PURE
    sharding_strategy: FULL_SHARD
    state_dict_type: sharded
    use_orig_params: false
    verbose: false
